---
title: "Simulation for genetic drift"
author: "Jinliang Yang"
date: "01-20-2020"
output:
  html_notebook: default
---

# Wright-Fisher simulation

Consider a single nucleotide position with two alleles, $A_1$ and $A_2$

In generation $t$ there are $x$ individuals carrying allele $A_1$, which is at frequency $p_t = x/2N$.

This implies that there are $2N - x$ individuals carrying allele $A_2$, which is at frequency $q_t = 1- p_t= 1- x/2N$.

The sampling of alleles for the next generations is equivalent to sampling from a binomial distribution with parameters size = $2N$ and prob= $x/2N$.

Therefore, the mean and variance of $p$ in the next generation for the Wright-Fisher model are:


\begin{align*}
E(p_t) &= p_t \\
Var(p_{t+1}) &= p_tq_t/2N 
\end{align*}


---

# Wright Fisher simulation


```{r}
wright_fisher <- function(N=1000, t=100, A1=100){
  # N: number of diploid individuals. [N=1000, integer]
  # t: number of generations. [t=100, numeric]
  # A1: number of A1 (the minor allele) in the first generation. [A1=10, integer]

  # create a vector 
  x <- 0
  x[1] <- A1

  # Start to loop over m generations
  # binomial sampling at each generation - determines allele count in next generations
  for (i in 2:t){
    # calculate allele freq in the current generation
    k <- (x[i-1])/(2*N)
    # generate a vector with 2N individuals
    n <- seq(0,2*N,1)
    
    # density for the binomial distribution with paramters size=2N and prob=k
    prob <- dbinom(n, 2*N, k)
    # plot(prob)
    # choose one from a vector each with certain prob
    x[i] <- sample(0:(2*N), 1, prob=prob)
  }
  # return back the results
  return(x)
}

# sim1 <- wright_fisher(N=50, t=100, A1=20)
```

---

```{r}
set.seed(12345879)
sim1 <- wright_fisher(N=50, t=10000, A1=20)
plot(sim1[1:100], type="o", pch=19, xlab="Generations", ylab="A1 allele count")

sim1 <- wright_fisher(N=50, t=1000, A1=20)
plot(sim1[1:100], type="o", pch=19, xlab="Generations", ylab="A1 allele count")

```


```{r}
set.seed(123456)
sim1 <- wright_fisher(N=50000, t=100, A1=20000)
plot(sim1[1:100], type="o", pch=19, xlab="Generations", ylab="A1 allele count")
```


---------

# Conducting data analysis using HCC

To make your life a little easier, you can add the following to `~/.ssh/config`:

```{bash, eval=FALSE}
Host crane
    HostName crane.unl.edu
    User username
```

## Getting to know slurm

[Slurm](https://slurm.schedmd.com/overview.html) is job managing system: you submit jobs via batch scripts. These batch scripts have common headers; we will see one below.

```{bash, eval=FALSE}
sinfo # view information about Slurm nodes and partitions.
sinfo --help
```

Here we see our `PARTITION` and `TIME LIMIT`, and all of their inferior but still useful friends.

Note that there is a column of `STATE`, which indicates the state of the machine. A better way of looking at what’s going on on each machine is with `squeue`, which is the job queue.

```{bash, eval=FALSE}
squeue --help
# view information about jobs located in the Slurm scheduling queue.
squeue 
squeue | wc -l # how many jobs are running
squeue | grep "jyanglab"
```

This shows each job ID (very important), partition the job is running on, name of person running the job. Also note `TIME` which is how long a job has been running.

This queue is very important: it can tell us who is running what where, and how long it’s been running. Also, if we realize that we’re accidentally doing something silly like mapping maize reads to the human genome, we can use `squeue` to find the job ID, allowing us to cancel a job with `scancel`. Let’s kill jyang21’s job:

```{bash, eval=FALSE}
scancel JOBID
```

# Warnings!!!

__Do not run anything on the headnode__ except cluster management tools (squeue, sbatch, etc), compilation tasks (but usually ask CSE help for big apps), or downloading files. If you run anything on the headnode, you will disgrace your lab. How embarrassing is this? Imagine if you had to give your QE dressed up like Richard Simmons. It’s that embarrassing.

__Monitor your disk space__, as it can fill up quickly.


# Using R with slurm

Often, we need to work with `R` interactively on a server. To do this, we use `srun` with the following options:


```{bash, eval=FALSE}
srun --nodes=1 --mem 4G --ntasks=4 --licenses=common --time=8:00:00 --pty bash
```

```{bash, eval=FALSE}
R # it wouldn't work
module load R/3.5
module avail

```

Run simulation on the remote cluster.

```{r, eval=FALSE}
set.seed(12345879)
sim1 <- wright_fisher(N=50, t=10000, A1=20)
write.table(sim1, "cache/sim1_n50_t10000.txt", sep="\t", row.names=FALSE, quote=FALSE)
```

Now back to the local computer

```{r, eval=FALSE}
sim1 <- read.table("../../cache/sim1_n50_t10000.txt", header=TRUE)

plot(sim1$x, type="o", pch=19, xlab="Generations", ylab="A1 allele count")
```




## Unix Commands

- `cd`: change the working directory
- `mkdir`: make directories
- `pwd`: print name of current working directory
- `ls`: list directory contents
- `chmod`: change the access permissions to files and directories
- `head`: output the first part of files
- `tail`: output the last part of files
- `more` and `less`: display contents of large files page by page or scroll line by line up and down
- `cat`: concatenate files
- `paste`: merge lines of files
- `wc`: print line, word, and byte counts for each each file
- `grep`: print lines matching a pattern
- `|`: pipe, i.e., `ls -la | head >> new_file`



-----------------------------

## An Example Slurm Batch Script Header

We wrap our jobs in little batch scripts, which is nice because these also help make steps reproducible. We’ll see how to write batch scripts for Slurm in the next section, but suppose we had one written called `steve.sh`. To keep your directory organized, I usually keep a scripts directory (or even `slurm-script/` if you have lots of other little scripts).

In each project directory, I make a directory called `slurm-log` for Slurm’s logs. Tip: use these logs, as these are very helpful in debugging. I separate them from my project because they fill up directories rather quickly.

Let’s look at an example batch script header for a job called `steve` (which is run with script `steve.sh`) that’s in a project directory.

```{bash, eval=FALSE}
#!/bin/bash -l
#SBATCH -D ~projects/your-cool-project/
#SBATCH -o ~/your-cool-project/slurm-log/steve-stdout-%j.txt
#SBATCH -e ~/your-cool-project/slurm-log/steve-stderr-%j.txt
#SBATCH -J steve
#SBATCH -t 24:00:00
set -e
set -u

# insert your script here
```



- `D` sets your project directory.
- `o` sets where standard output (of your batch script) goes.
- `e` sets where standard error (of your batch script) goes.
- `J` sets the job name.
- `t` sets the time limit for the job, 24:00:00 indicates 24 hours.

Note that the programs in your batch script can redirect their output however they like — something you will like want to do. This is the standard output and standard error of the batch script itself.

Also note that these directories must already be made — Slurm will not create them if they don’t exist. If they don’t exist, sbatch will not work and die silently (since there’s no place to write standard error). If you keep trying something and it doesn’t log the error, make sure all these directories exist.

As mentioned, the `jobname` is how you distinguish your jobs in squeue. If we ran this, we’d see “steve” in the `JOBS` column.

The time limit for the job should be greater than the estimated time to complete your job. Time-and-a-half or twice as much time as you think it will take are good rules. If your job reaches this time limit it will be killed. It’s frustrating to lose a job because you underestimate the time. Alternatively, you can set this with the –time flag (instead of -t, e.g. –time=1-00:00 sets a time limit of one day).

## An example script

Try running this test script:

```{bash, eval=FALSE}
#!/bin/bash -l
#SBATCH -D /home/USERNAME
#SBATCH -J bob
#SBATCH -o /home/USERNAME/out-%j.txt
#SBATCH -e /home/USERNAME/error-%j.txt
#SBATCH -t 24:00:00
#SBATCH --array=0-8

bob=( 1 1 1 2 2 2 3 3 3 )
sue=( 1 2 3 1 2 3 1 2 3 )

block=${bob[$SLURM_ARRAY_TASK_ID]}
min=${sue[$SLURM_ARRAY_TASK_ID]}

echo "$block is $min" 
```

Make sure you switch your user name for `USERNAME`. You should see a bunch of files named “error” and “out” show up in your home directory. 


















